quantization_config:
  load_in_4bit: true
  bnb_4bit_compute_dtype: torch.float16

model_id: "llava-hf/llava-1.5-7b-hf"
